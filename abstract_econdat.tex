Humans have a tendency to see `human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). 
In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. 
We focus on example applications in the field of Economics and Finance, discussing both the benefits and pitfalls of common approaches to mechanistic interpretability. Our most extensive example involves a novel financial dataset and LLM that have been proposed for quantifying the `hawkishness' of central bank communication. We demonstrate that even though standard tools from mechanistic interpretability yield favourable outcomes for the model, it still evidently lacks any sort of `understanding' of the economy. 
Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.