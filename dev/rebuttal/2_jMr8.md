# Reviewer 2 (jMr8, rating: 6 `weak accept')

We thank the reviewer for the detailed and thoughtful review. 

## Section 2.1 - Limitations of Inverse Problem

The reviewer pointed out that the "inverse problem is ill-conditioned and could not recover the perfect signal in theory." The authors agree that this can indeed be seen as a form of inverse problem and common caveats such as non-uniqueness and instability apply ([Haltmeier and Nguyen, 2020](https://arxiv.org/abs/2006.03972)). We use $\ell^2$-regularization for the linear probe (ridge regression), which corresponds to Tikhonov regularization in the context of approximation theory ([Björkström, 2001](https://www2.math.su.se/matstat/reports/seriea/2000/rep5/report.pdf)). We confess that we did not carefully consider the parameter choice for the ridge penalty, nor has this been carefully studied in the broader literature on linear probes for mechanistic interpretability to the best of our knowledge.
Indeed we would expect that the signal would not perfectly be recovered in theory given the compression of the data. However, some signal would indeed imperfectly be recovered, with a closer approximation of perfection in recovery the 'better' the model is. Given that LLMs are essentially compressing textual information, and that LLMs have demonstrated a high capability of doing so, we of course expect an inverse attempt to recover the signal to be partially successful. What is not to be expected is an interpretation that this imperfect yet-close-enough-to-accurate signal recovery is interpreted as the system having a 'mental model' of the earth, and thus showing 'sparks' of general intelligence - especially when 'general intelligence' and even 'intelligence' are narrowly defined in this space, and when there are ideal conditions for misinterpretation of results in the form of anthropomorphism and cognitive bias. 
Thus, we still think that this simple experiment in Section 2.1 describes the problem of running linear probes on LLMs reasonably well at a high level. If anything, the challenges pointed out by the reviewer therefore cast further doubt on this practice. We will add a note or paragraph on this in the final manuscript. Furthermore, we would like to point out that even in the case of imperfect yet-close-enough-to-accurate signal recovery, this is not to be interpreted as the system having a 'mental model' of the earth, and thus showing 'sparks' of general intelligence - especially when 'general intelligence' and even 'intelligence' are narrowly defined in this space, and when there are ideal conditions for misinterpretation of results in the form of anthropomorphism and cognitive bias.

## Section 2.3.2. - The Parrot Test

Section 2.3.2. applied a 'strong' test to the hypothesis that LLMs 'understand'. We show results that one might interpret as the models having some understanding of economics, and being able to imperfectly forecast economic indicators - a parallel to the notion that model output correlating with latitude longitude is an indication that the model ‘understands’ geography, or has a ‘mental’ model of the globe. 
The 'spurious' correlation then, is in the interpretation that the 'accuracy' of the output of the model correlating with economic indicators is a cue that the model is understanding - or more specifically that the correlation between the output of the model and the 'ground truth' economic indicators is 'caused' by the model's understanding of the text. 
We then show that the model fails to 'understand' when we apply a simple linguistic transformation to our experiment. 
## Section 3.2 - Anthropomorphism and AI
Your recommendation that we better ground sec 3.2 in the current research space is well-noted. Reflecting on your comment, we think there are at least 3 ways to reach a better understanding:
Acknowledgement: researchers should consider acknowledging our tendency to anthropomorphize either in a dedicated ‘limitations’ section or when discussing results.
Stronger testing: researchers should refrain from drawing premature conclusions about AGI, unless these conclusions are based on strong hypothesis tests.
Epistemologically robust standards: we call for more precise definitions of terms like ‘intelligence’ and ‘AGI’, and iterations over how we will measure them
We feel an appropriate discussion would require more room than is allocated in the paper. Nevertheless, we can attempt to summarize our position in the main body or perhaps in the appendix.
For a slightly more detailed discussion, please see our comments below:
A crucial first step is to acknowledge openly that such cognitive bias and tendency towards anthropomorphism exists, and is especially likely in this space. While it may remain in the language that we use (e.g. phrases like the model ‘thinks’, ‘understands’, etc.), behaviorally it must be addressed in the research space, e.g. with acknowledgment in ‘limitations’ sections of papers, and in the interpretation of results in our discussion sections. This may lead to more broad attempts to address anthropomorphism, e.g. with better research designs and analyses.
A second step, and one such research design element, is stronger testing which we attempt in this paper. Intelligence is only ‘indirectly’ measurable in automated systems as it is in humans. Thus we must also acknowledge that claims of ‘AGI’ deserve scrutiny. If such a claim is easily negated with a simple change in language, we take this as a sign that the field is only weakly testing its claims. We note that this does not mean that researchers are not making great efforts to test their models - leaderboards on Huggingface, and the limitless torrent of papers on LLMs would speak to the contrary. However, these tests may not be appropriate indicators that a system is approaching AGI, but rather that a system is performing well on some narrowly defined task and accompanying dataset. Thus, if we are to approach AGI with our work, we must have a strong test - a risky test that a system is likely to fail. In fields like psychology where the concept of intelligence itself emerged, discussions of appropriately strong tests for claims abound - we require the same rigor in this field as well. 
Thirdly, and more broadly, we see a need for more epistemologically robust standards in the work itself to combat anthropomorphism. To this end, further inspiration may be drawn from psychology - ‘construct’ oriented definitions of important concepts, and a focus on achieving better indirect measurement. While computer science has traditionally not been a hypothesis-driven field that deals with indirect and imperfect measurements, psychology research methods have evolved specifically for these purposes. Applying this to the study of AGI requires first a somewhat-agreed upon definition not only on what ‘AGI’ is, but also how it would manifest itself - with the understanding that it may appear differently than intelligence does in humans. 
Computer science tends to borrow heavily from the rather vague definition of Spearman that it is about ‘generally solving problems (e.g. [Goertzel, 2014](https://sciendo.com/article/10.2478/jagi-2014-0001)) - if we wish to take advantage of this definition, we might attempt to already claim that the LLMs achieve this: the models weren’t designed for any specific task, and yet can be used to assist in many tasks. At the same time, our work shows that this does not mean that these models ‘understand’ the problems they are solving, nor can they formulate the problems in the first place. If our current assessments are all inspired by how we measure intelligence in humans, and models are trained on the material of the tests and appropriate responses, assessing ‘AGI’ in this way is simply inappropriate. We don’t have a good answer to this last point - but are excited to consider what a field that acknowledges its own biases can accomplish in this regard. 

## Other Modalities

From what we have observed in the past, we think the concerns we have raised may apply to many applications, but are urgent in the context of LLMs. Many people attribute our ability to  understand, to our ability to communicate through language. Recent implementations as chat bots, e.g. Replika and Chat GPT, as well as virtual assistants (Alexa, Cortana) have afforded a kind of interaction that resembles linguistic exchanges with other humans. We can certainly imagine there is some degree of anthropomorphizing with any ‘AI’ technology that has some apparent behavior e.g. self-driving cars. But as language is an all too obvious feature distinguishing us from other species, it is not surprising that LLMs have so far been anthropomorphized more so than foundation models with other modalities - language is what makes them an ingredient in the ‘perfect storm’. 
