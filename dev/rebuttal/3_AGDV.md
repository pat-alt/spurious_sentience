# Reviewer 3 (AGDV, rating: 7 `accept')

We thank the reviewer for the detailed and thoughtful review. 

## Revised Abstract

The reviewer pointed out that the abstract was not optimally aligned with the paper and we agree that there is some room for improvement. We propose this more detailed and longer revised version:

“Developments in the field of AI in general, and Large Language Models (LLMs) in particular, have created a ‘perfect storm’ for observing ‘sparks’ of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill representations in their latent embeddings that have been shown to correlate with meaningful phenomena. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying degrees of sophistication including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill knowledge and yet none of them develop true understanding. Specifically, we show that embeddings of a language model fine-tuned on central bank communications can make meaningful predictions, via correlations with unseen economic variables, such as price inflation. However, we then show that inflation is also predicted for nonsense prompts about growing and shrinking bird populations (‘dovelation’). We therefore argue that patterns in latent spaces are spurious sparks of (AGI). Additionally, we review literature from the social sciences that shows that humans are prone to seek patterns and anthropomorphize. We, therefore, argue that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying ‘ground truth’ relationships. We therefore call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.”

## Cognitive Biases

While we appreciate the feedback, we would like some clarity on what precisely this reviewer means by ‘a bit poorly argued’? We do indeed see that 366-371 can be cut. We may have included details that summarize more of the material than is essential to the discussion, and perhaps trimming would make our arguments more clear. Nevertheless, we feel the included text summarizes work that has shown antecedents to two forms of cognitive bias. These sections aim to show how the presence of the antecedents makes the current situation with ‘AI’ tools in general and with LLMs in specific, a perfect storm for misinterpretation. We would happily take on any suggestions as to how this can be clearer and better argued. 

## Figure 3

We apologize for the somewhat sloppy figure description and appreciate the reviewer’s suggestions. The solid line is just a smooth trendline, which we added to highlight that we find the expected negative relationship between estimated errors and network depth. On second thought, and in light of your feedback, we will simply remove these trend lines to avoid confusion. We will adjust the figure caption as follows: “Out-of-sample root mean squared error (RMSE) for the linear probe plotted against FOMC-RoBERTa’s n-th layer for different indicators. The values correspond to averages computed across cross-validation folds, where we have used an expanding window approach to split the time series. As expected, we observe that model performance tends to be higher (average prediction errors are lower) for layers near the end of the transformer model.”

Although the errors are indeed low compared to baseline autoregressive models (as we explain in lines 237-249), this chart is not the most obvious choice to drive home that particular point. We chose to highlight this chart nonetheless because it is more consistent with the presentations chosen in the related literature on mechanistic interpretability. It is common to show that probe performance improves for layer activations near the output layer, which illustrates that “neural networks are really about distilling computationally useful *representations*” as opposed to *information contents* ([Alain and Bengio, 2018](https://arxiv.org/abs/1610.01644)). 

## Confusing Title

We appreciate this take and understand that this sort of title may not be appealing to everyone. To avoid any confusion about a potential typo, we have addressed this in the revised abstract. We are still keen to keep the title as is unless the reviewer(s) find(s) it critical that we adjust it.
