%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\RequirePackage[hyphens]{url}
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
 \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Position Paper: Against Spurious Sparks---Dovelating Inflated AI Claims}

\begin{document}

\twocolumn[
\icmltitle{Position Paper:\\Against Spurious Sparks---Dovelating Inflated AI Claims \raisebox{-6pt}{\includegraphics[scale=0.09]{dove_1f54a-fe0f.png}}}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Patrick Altmeyer}{equal,yyy}
\icmlauthor{Andrew M. Demetriou}{equal,yyy}
\icmlauthor{Antony Bartlett}{yyy}
\icmlauthor{Cynthia C. S. Liem}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands}

\icmlcorrespondingauthor{Patrick Altmeyer}{patrick.altmeyer@tudelft.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Anthropomorphism, Artificial General Intelligence}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Humans have a tendency to see `human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). 
In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs.
%why humans invariably anthropomorphize objects, and how the search for Artificial General Intelligence (AGI) is a perfect storm to overemphasize the performance of LLMs. To combat this,
In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.
%Finally, we discuss how the popularity in media and the race for AGI enforces spurious over-interpretation of intelligence in the models we use.

\end{abstract}

\section{Introduction}
\label{submission}

% \textcolor{magenta}{TODOs for the team:
% \begin{itemize}
%     \item Please double check the citations. Note this style file does not retain capitalization in titles. A trick to retain capitalization is to put an extra set of brackets around the title. I did this for the Ladouceur et al. and Bubeck et al references already; please make this consistent throughout.
%     \item As another issue with the citations: note that arxiv preprints do not show very well. Best way to make them show is to wrap them as 'journal articles' as done in the Arora citation. Please straighten this out for the other articles too
% \end{itemize}
% }

In 1942, when anti-intellectualism was rising and the integrity of science was under attack, Robert K.\ Merton formulated four `institutional imperatives' as comprising the ethos of modern science: \emph{universalism}, meaning that the acceptance or rejection of claims entering the lists of science should not depend on personal or social attributes of the person bringing in these claims; \emph{``communism''} [sic], meaning that there should be common ownership of scientific findings and one should communicate findings, rather than keeping them secret; \emph{disinterestedness}, meaning that scientific integrity is upheld by not having self-interested motivations, and \emph{organized skepticism}, meaning that judgment on the scientific contribution should be suspended until detached scrutiny is performed, according to institutionally accepted criteria~\cite{merton1942science}. While the Mertonian norms may not formally be known to academics today, they still are implicitly being subscribed to in many ways in which academia has organized academic scrutiny; e.g., through the adoption of double-blind peer reviewing, and in motivations behind open science reforms.

At the same time, in the way in which academic research is disseminated in the AI and machine learning fields today, major shifts are happening. Where these research fields have actively adopted early sharing of preprints and code, the volume of publishable work has exploded to a degree that one cannot reasonably keep up with broad state-of-the-art, and social media influencers start playing a role in article discovery and citeability~\cite{weissburg2024tweets}. Furthermore, because of major commercial stakes with regard to AI and machine learning technology, and e.g.\ following the enthusiastic societal uptake of products employing large language models (LLMs), such as ChatGPT, the pressure to beat competitors as fast as possible is only increasing, and strong eagerness can be observed in many domains to `do something with AI' in order to innovate and remain current.

Where AI used to be a computational modeling tool to better understand human cognition~\cite{vanrooij2023aitheoretical}, the recent interest in AI and LLMs has been turning into one in which AI is seen as a tool that can mimic, surpass and potentially replace human intelligence. In this, the achievement of Artificial General Intelligence (AGI) has become a grand challenge, and in some cases, an explicit business goal. The definition of AGI itself is not as clear-cut or consistent; loosely, it is a phenomenon contrasting with `narrow AI' systems, that were trained for specific tasks~\cite{goertzel2014artificial}. In practice, to demonstrate that the achievement of AGI may be getting closer, researchers have sought to show that AI models generalize to different (and possibly unseen) tasks, with little human intervention, or show performance considered `surprising' to humans.

For example, Google DeepMind claimed their AlphaGeometry model~\cite{trinh2024geometry} reached a `milestone' towards AGI. This model has the ability to solve complex geometry problems, allegedly without the need for human demonstrations during training. However, work such as this had been initially introduced in the 1950s~\cite{hector2024linkedin}: without the use of an LLM, logical inference systems proved 100\% accurate in proving all the theorems of Euclidean Geometry, due to geometry being an axiomatically closed system. Therefore, while DeepMind created a powerfully fast geometry-solving machine, it is still far from AGI.%despite DeepMind's success in creating a powerfully fast geometry-solving machine, it is still far from being AGI.

Generally, in the popularity of ChatGPT and the integration of generative AI in productivity tools (e.g.\ through Microsoft's Copilot integrations in GitHub and Office applications), one also can wonder whether the promise of AI is more in computationally achieving general intelligence, or rather in the engineering of general-purpose tools\footnote{A Swiss army knife is an effective general-purpose tool, without people wondering whether it is intelligent.}. Regardless, stakes and interests are high, e.g.\ with ChatGPT clearing nearly \$1 billion in months of its release\footnote{\url{https://www.bloomberg.com/news/articles/2023-08-30/openai-nears-1-billion-of-annual-sales-as-chatgpt-takes-off}}.

When combining massive financial incentives with the presence of a challenging and difficult-to-understand technology, that aims towards human-like problem-solving and communication abilities, a situation arises that is fertile for the misinterpretation of spurious cues as hints towards AGI, or other qualities like sentience\footnote{\url{https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/}} and consciousness. AI technology only becomes more difficult to understand as academic publishing in the space largely favors performance, generalization, quantitative evidence, efficiency, building on past work, and novelty~\cite{values_in_ML}. As such, works that make it into top-tier venues tend to propose heavier and more complicated technical takes on tasks that (in the push towards generalizability) get more vague, while the scaling-up of data makes traceability of possible memorization harder. In a submission-overloaded reality, researchers may further get incentivized to oversell and overstate achievement claims.

Noticing these trends, we as the authors of this article are concerned. We feel that the current culture of racing toward Big Outcome Statements in industry and academic publishing too much disincentivizes efforts toward more thorough and nuanced actual problem understanding. At the same time, as the outside world is so eager to adopt AI technology, (too) strong claims make for good sales pitches, but a question is whether there is indeed sufficient evidence for these claims. With successful A(G)I outcomes needing to look human-like, this also directly plays into risks of anthropomorphizing (the attribution of human-like qualities to non-human objects) and confirmation bias (the seeking-out and/or biased interpretation of evidence in support of one's beliefs). In other words, it is very tempting to claim surprising human-like achievements of AI, and as humans, we are very prone to genuinely believing this.

To strengthen our argument, in this paper, we first consider a recently viral work~\cite{gurnee2023languagev2} in which claims about the learning of world models by LLMs were made. We present several experiments that may invite similar claims on models yielding more intelligent outcomes than would have been expected---
%, especially of a nature that they may look attractive for adoption as an innovative predictive tool in concrete application domains---while we at the same time indicate 
while at the same time indicating how we feel these claims should \emph{not} be made. Furthermore, we present a review of social science findings that underline how prone humans are to being enticed by patterns that are not really there. Combining this with the way in which media portrayal of AI has tended towards science-fiction imagery of mankind-threatening robots, we argue that the current AI culture is a perfect storm for making and believing inflated claims, and call upon our fellow academics to be extra mindful and scrutinous about this.

%Although this is a fantastic achievement by the team, where we observe the model outperforming previous state of the art (SOTA) models, the underlying model is far from AGI. As discussed in the paper \cite{trinh2024geometry}, AlphaGeometry utilizes a rule-bound deduction engine, allowing the use of an NLP model for further suggestions when it is unable to locate a logical answer. With this combination, it was able to score highly (\textit{25/30} questions solved) where the gold standard human equivalent was \textit{25.9/30}.

%One of the more contentious pieces of literature recently studied, is the paper by \cite{gurnee2023languagev2}. This paper, which was the inspiration for our work, received numerous points of critical feedback when they discussed their work on the X\footnote{https://twitter.com/wesg52/status/1709551516577902782?s=20} (formally Twitter) platform. Although not directly mentioning AGI, they do hint at the emergence of intelligence, specifically with the creation of the world model.


%This only gets amplified as the interest in Artificial General Intelligence has now merged with a push towards building general-purpose tools.

%\textcolor{magenta}{cynthia: I am now moving the texts of Andrew and Antony on AGI here; maybe some tightening can already be done in this. Unfortunately no time for thorough editing yet at this stage, but will come back to this later}

%In the current race to build the first Artificial General Intelligence (AGI) system, loosely defined as an intelligent system that solve general problems rather than narrow, specific ones, and that demonstrates the abilities of reasoning, planning, and learning from past experience at the level of a human or better e.g. \cite{bubeck2023sparks}, ChatGPT cleared nearly \$1 billion in months of its release according to \href{https://www.bloomberg.com/news/articles/2023-08-30/openai-nears-1-billion-of-annual-sales-as-chatgpt-takes-off}{Bloomberg}, the paid version of which OpenAI claimed only showed 'sparks of AGI'. 

%Present work case studies an example, to which claims of observed AGI 'sparks' in AI research, and reviews work on two human phenomena that are likely to lead to false claims: anthropomorphism, or the attribution of human-like qualities to non-human objects, and confirmation bias, or the seeking-out and/or biased interpretation of evidence in support of one's beliefs. We describe how these phenomena may interact in the context of AI research and development, leading to especially poor inferences about the degree of general intelligence, or lack thereof, within AI systems. We further display results from experiments that at first appear to indicate nascent 'sparks' of AGI, but that stronger tests show are indications of spurious sparks.

%As previously mentioned, AGI is the current objective for which the global AI community is shooting for. As observed in 2023, the mainstream implementation of Large Language Models (LLMs) resulted in numerous societal impacts. This enabled those companies with the more mature models to reap considerable monetary returns, despite the numerous negative news feeds. True AGI will be another global milestone for AI and the entire scientific community, thus we already witness massive claims towards it.

% Recently, Google DeepMind made claims their AlphaGeometry model \cite{trinh2024geometry} reached a 'milestone' towards AGI. This model has the ability to solve complex geometry problems, which they state it does without the need for human demonstrations during training. Although this is a fantastic achievement by the team, where we observe the model outperforming previous state of the art (SOTA) models, the underlying model is far from AGI. As discussed in the paper \cite{trinh2024geometry}, AlphaGeometry utilizes a rule-bound deduction engine, allowing the use of an NLP model for further suggestions when it is unable to locate a logical answer. With this combination, it was able to score highly (\textit{25/30} questions solved) where the gold standard human equivalent was \textit{25.9/30}.

% However, as noted by Hector Zenil \cite{hector2024linkedin}, that work such as this had been initially introduced in the 1950s. Without the use of an LLM, logical inference systems proved 100\% accurate in proving all the theorems of Euclidean Geometry. They were able to achieve this success due to geometry being an axiomatically closed system. Therefore, despite DeepMinds success in creating a powerfully fast Geometry solving machine, it is still far from being an AGI.

%it is perhaps not surprising to see human-metaphors in how AI has been designed and described, even though these very designs prevent it from ever having the cognitive functions of a human e.g. \cite{salles2020anthropomorphism, van2023reclaiming}. 

\section{Surprising Patterns in Latent Spaces?}\label{patterns-in-latent-spaces-and-how-to-find-them}

In 2023, a research article went viral on the X\footnote{\url{https://twitter.com/wesg52/status/1709551516577902782?s=20}}
%(formerly Twitter)
platform~\cite{gurnee2023languagev1}. Through linear probing experiments, the claim was made that LLMs learned literal maps of the world. As such, they were considered to be more than `stochastic parrots'~\cite{bender2021dangers} that can only correlate and mimic existing patterns from data, but not truly understand it. While the manuscript immediately received public criticism~\cite{marcus2023muddles}, and the revised, current version is more careful with regard to its claims~\cite{gurnee2023languagev2}, reactions on X seemed to largely exhibit excitement and surprise at the authors' findings. However, in this section, through various simple examples, we make the point that observing patterns in latent spaces should not be a surprising revelation. After starting with a playful example of how easy it is to `observe' a world model, we build up a larger example focusing on key economic indicators and central bank communications.

%One of the more contentious pieces of literature recently studied, is the paper by \cite{gurnee2023languagev2}. This paper, which was the inspiration for our work, received numerous points of critical feedback when they discussed their work on the X\footnote{\url{https://twitter.com/wesg52/status/1709551516577902782?s=20}} (formally Twitter) platform. Although not directly mentioning AGI, they do hint at the emergence of intelligence, specifically with the creation of the world model.

%They claim said models generate an internal representation of the world, with which spatiotemporal representations are created. Further to this we see claim to the generalization of tasks, in which the model can answer questions regarding spatial and temporal relationships, without the need for further training. Although these can be seen as building blocks for a more comprehensive model, it is also fair to assume that it is mere conjecture and more likely spurious correlations.

%In this section, we navigate a number of simple examples to support our claim, that observing patterns in latent spaces should not be a surprising revelation.
%To support our claim that observing patterns in latent spaces should not generally surprise us, we will now go through a couple of simple examples. 
%We will draw on ideas from the fields of Economics and Finance, to illustrate further that this phenomenon is neither surprising nor unique to the field of computer science. We will start with very simple examples to demonstrate that even small and simple models can learn meaningful representations of the data. The final example in Section \ref{ex-llm} is more complex and closer in spirit to the experiments conducted by \citet{gurnee2023language}. As we progress, we will discuss both the benefits and potential pitfalls of finding patterns in latent spaces.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth]{results/figures/map.png}

}

\caption{\label{fig-map}Predicted coordinate values (in-sample) from a linear probe on final-layer activations of an untrained neural network.}

\end{figure}

\subsection{Are Neural Networks Born with World Models?}\label{example-deep-learning}

\citet{gurnee2023languagev2} extract and visualize the alleged geographical world model by training linear regression probes on internal activations in LLMs (including Llama-2) for the names of places, to predict geographical coordinates associated with these places. Now, the Llama-2 model has ingested huge amounts of publicly available data from the internet, including Wikipedia dumps from the June-August 2022 period \citep{touvron2023llama}. It is therefore highly likely that the training data contains geographical coordinates, either directly or indirectly. At the very least, we should expect that the model has seen features during training that are highly correlated with geographical coordinates. The model itself is essentially a very large latent space to which all features are randomly projected in the very first instance before being passed through a series of layers which are gradually trained for downstream tasks.

In our first example, we simulate this scenario, stopping short of training the model. In particular, we take the \href{https://github.com/wesg52/world-models/blob/main/data/entity_datasets/world_place.csv}{world\_place.csv} that was used in \citet{gurnee2023languagev2}, which maps locations/areas to their latitude and longitude. For each place, it also indicates the corresponding country. From this, we take the subset that contains countries that are currently part of the top 10 \href{https://www.fifa.com/fifa-world-ranking/men?dateId=id14142}{FIFA world ranking}, and assign the current rank to each country (i.e., Argentina gets 1, France gets 2, \ldots{}). To ensure that the training data only involves a noisy version of the coordinates, we transform the longitude and latitude data as follows: \(\rho \cdot \text{coord} + (1-\rho) \cdot \epsilon\) where \(\rho=0.5\) and \(\epsilon \sim \mathcal{N}(0, 5)\).

Next, we encode all features except the FIFA world rank indicator as continuous variables: \(X^{(n \times m)}\) where \(n\) is the number of samples and \(m\) is the number of resulting features. Additionally, we add a large number of random features to \(X\) to simulate the fact that not all features ingested by Llama-2 are necessarily correlated with geographical coordinates. Let \(d\) denote the final number of features, i.e.~\(d=m+k\) where \(k\) is the number of random features.

We then initialize a small neural network, considered a \emph{projector}, mapping from \(X\) to a single hidden layer with \(h<d\) hidden units and sigmoid activation, and from there, to a lower-dimensional output space. Without performing any training on the \emph{projector}, we simply compute a forward pass of \(X\) %through it
%the \emph{projector}
and retrieve activations \(\mathbf{Z}^{(n\times h)}\). Next, we perform the linear probe on a subset of \(\mathbf{Z}\) through Ridge regression: \(\mathbf{W} = (\mathbf{Z}_{\text{train}}'\mathbf{Z}_{\text{train}} + \lambda \mathbf{I}) (\mathbf{Z}_{\text{train}}'\textbf{coord})^{-1}\), where \(\textbf{coord}\) is the \((n \times 2)\) matrix containing the longitude and latitude for each sample. A hold-out set is reserved for testing, on which we compute predicted coordinates for each sample as \(\widehat{\textbf{coord}}=\mathbf{Z}_{\text{test}}\mathbf{W}\) and plot these on a world map ( Figure~\ref{fig-map}).  
%. Finally, we compute the predicted coordinates for each sample in the hold-out set as \(\widehat{\textbf{coord}}=\mathbf{Z}_{\text{test}}\mathbf{W}\) and plot the results on a world map (see Figure~\ref{fig-map}).  

While the fit certainly is not perfect, the results do indicate that the random projection contains representations that are useful for the task at hand. Thus, %with this simple example, we
this simple example illustrates that meaningful target representations should be recoverable from a sufficiently large latent space, given the projection of a small number of highly correlated features. Similarly, \citet{alain2018understanding} observe that even before training a convolutional neural network on MNIST data, the layer-wise activations can already be used to perform binary classification. In fact, it is well-known that random projections can be used for prediction tasks \cite{dasgupta2013experiments}.


\subsection{PCA as a Yield Curve Interpreter}\label{example-principal-component-analysis}
%\subsection{Example: Principal Component Analysis}\label{example-principal-component-analysis}

We now move to a concrete application domain: Economics. Here, the yield curve, plotting the yields of bonds against their maturities, is a popular tool for investors and economists to gauge the health of the economy.
%It plots the yields of bonds against their maturities.
The yield curve's slope is often used as a predictor of future economic activity: a steep yield curve is associated with a growing economy, while a flat or inverted yield curve is associated with a contracting economy. %While for many intents and purpose it may be sufficient to look at the yield curve of US Treasury bonds, the broader universe of traded fixed-income instruments is large and may contain useful information that is not captured by US Treasuries alone.
To leverage this information in downstream modelling tasks, economists regularly use PCA to extract a low-dimensional projection of the yield curve that captures relevant variation in the data (e.g.\ \citet{berardi2022dissecting}, \citet{kumar2022effective} and \citet{crump2019deconstructing}).

%Being primarily interested in understanding the nature of this low-dimensional projection, we limit our scope to the US Treasury yield curve. To this end, we
To understand the nature of this low-dimensional projection, we collect daily Treasury par yield curve rates at all available maturities from the US Department of the Treasury. Computing principal components involves decomposing the matrix of all yields \(\mathbf{r}\) into a product of its singular vectors and values: \(\mathbf{r}=\mathbf{U}\Sigma\mathbf{V}^{\prime}\). Let us simply refer to \(\mathbf{U}\), \(\Sigma\) and \(\mathbf{V}^{\prime}\) as latent embeddings of the yield curve.

The upper panel in Figure~\ref{fig-pca-yield} shows the first two principal components of the yield curves of US Treasury bonds over time. Vertical stalks indicate key dates related to the Global Financial Crisis (GFC). During its onset, on 27 February 2007, financial markets were in turmoil following a warning from the Federal Reserve (Fed) that the US economy was at risk of a recession. The Fed later reacted to mounting economic pressures by gradually reducing short-term interest rates to unprecedented lows. Consequently, the average level of yields decreased and the curve steepened. In Figure~\ref{fig-pca-yield}, we can observe that the first two principal components appear to capture this level shift and steepening, respectively. In fact, they are strongly positively correlated with the actual observed first two moments of the yield curve (lower panel of Figure~\ref{fig-pca-yield}).


\begin{figure*}[tb]

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/pca_yield.png}

}

\caption{\label{fig-pca-yield}Top chart: The first two principal components of US Treasury yields over time at daily frequency. Bottom chart: Observed average level and 10yr-3mo spread of the yield curve. Vertical stalks roughly indicate the onset ($|$GFC) and the beginning of the aftermath (GFC$|$) of the Global Financial Crisis.}

\end{figure*}%

%Of course, even though it is reassuring to see that these latent embeddings are meaningful, none of this is surprising at all: 
Again, it should not be surprising that these latent embeddings are meaningful: by construction, principal components are orthogonal linear combinations of the data itself, each of which explains most of the residual variance after controlling for the effect of all previous components.

%\subsection{Example: Large Language Model}\label{ex-llm}
\subsection{LLMs for Economic Sentiment Prediction}\label{ex-llm}

So far, we considered simple linear data transformations. One might argue that this does not really involve latent embeddings in the way they are typically thought of in the context of deep learning. In Appendix~\ref{appendix:autoencoder}, we present an additional experiment in which we more explicitly seek neural network-based representations that will be useful for downstream tasks. Here, we continue with an example in which LLMs may be used for Economic Sentiment Prediction.

Closely following the approach in \citet{gurnee2023languagev2}, we apply it to the novel \emph{Trillion Dollar Words}~\cite{shah2023trillion} financial dataset, containing a curated selection of sentences formulated by central bankers of the US Federal Reserve, and communicated to the public in speeches, meeting minutes and press conferences. ~\cite{shah2023trillion} use this dataset to train a set of LLMs and rule-based models to classify sentences as either `dovish', `hawkish' or `neutral'. In the context of central banking, `hawkishness' is typically associated with tight monetary policy: in other words, a `hawkish' stance on policy favors high interest rates to limit the supply of money and thereby control inflation. The authors first manually annotate a sub-sample of the available data and then fine-tune various models for the classification task. Their model of choice, \emph{FOMC-RoBERTa} (a fine-tuned version of RoBERTa \citep{liu2019roberta}), achieves an \(F_1\) score of around \(>0.7\) on the test data. To illustrate the potential usefulness of the learned classifier, they use predicted labels for the entire dataset to compute an ad-hoc, count-based measure of `hawkishness'. This measure is shown to correlate with key economic indicators in the expected direction: when inflationary pressures rise, the measured level of `hawkishness' increases, as central bankers react by raising interest rates to bring inflation back to target.

\subsubsection{Linear Probes}\label{linear-probes}
%Instead of computing a measure based on predicted labels,
We now use linear probes to assess if the fine-tuned model has learned associative patterns between central bank communications and key economic indicators. Therefore, we further pre-process the data provided by \citet{shah2023trillion} and use their proposed model to compute activations of the hidden state, on the first entity token for each layer. We have made these available and easily accessible through a small Julia package: \href{https://anonymous.4open.science/r/TrillionDollarWords/README.md}{TrillionDollarWords.jl}. 

For each layer, we compute linear probes through Ridge regression on two inflation indicators (the Consumer Price Index (CPI) and the Producer Price Index (PPI)) and US Treasury yields at different levels of maturity. To allow
%for a 
comparison with \citet{shah2023trillion}, we let yields enter the regressions in levels. To measure price inflation we use percentage changes proxied by log differences. To mitigate issues related to over-parameterization, we follow the recommendation in \citet{alain2018understanding} to first reduce the dimensionality of the computed activations.% each time.
In particular, we restrict our linear probes to the first 128 principal components of the embeddings of each layer.
%We found that not doing so would generally lead to poor out-of-sample performance.
To account for stochasticity, we use an expanding window scheme with 5 folds for each indicator and layer. To avoid look-ahead bias, PCA is always computed on the sub-samples used for training the probe. 

Figure~\ref{fig-fomc} shows the out-of-sample root mean squared error (RMSE) for the linear probe, plotted against \emph{FOMC-RoBERTa}'s \(n\)-th layer. The values correspond to averages across cross-validation folds. Consistent with related work~\citep{alain2018understanding,gurnee2023languagev2}, we observe that model performance tends to be higher for layers near the end of the transformer model. Curiously, for yields at longer maturities, we find that performance eventually deteriorates for the very final layers. We do not observe this for the training data, so we attribute this to overfitting. 

% After all we are still regressing a single variable of interest on 128 regressors.

It should be noted that performance improvements are generally of small magnitude. Still, the overall qualitative findings are in line with expectations. Similarly, we also observe that these layers tend to produce predictions that are more positively correlated with the outcome of interest and achieve higher mean directional accuracy (MDA). Upon visual inspection of the predicted values, we conclude the primary source of prediction errors is low overall sensitivity, meaning that the magnitude of predictions is generally too small. 

To better assess the predictive power of our probes, we compare their predictions to those made by simple autoregressive models. For each layer, indicator and cross-validation fold, we first determine the optimal lag length based on the training data using the Bayes Information Criterium with a maximal lag length of 10. These are not state-of-the-art forecasting models, but they serve as a reasonable baseline. For most indicators, probe predictions outperform the baseline in terms of average performance measures. After accounting for variation across folds, however, we generally conclude that the probes neither significantly outperform nor underperform. Detailed results, in which we also perform more explicit statistical testing, can be found in Appendix~\ref{appendix:parrot}.


\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/rmse_pca_128.png}

}

\caption{\label{fig-fomc}Out-of-sample root mean squared error (RMSE) for the linear probe plotted against \emph{FOMC-RoBERTa}'s \(n\)-th layer for different indicators. The values correspond to averages computed across cross-validation folds, where we have used an expanding window approach to split the time series.}

\end{figure*}%

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/attack_all_measures.png}

}

\caption{\label{fig-attack}Probe predictions for sentences about inflation of prices (IP), deflation of prices (DP), inflation of birds (IB) and deflation of birds (DB). The vertical axis shows predicted inflation levels subtracted by the average predicted value of the probe for random noise.}

\end{figure*}%

%\subsubsection{Stochastic Parrots AfterAll?}\label{stochastic-parrots-after-all}
\subsubsection{Sparks of economic understanding?}\label{stochastic-parrots-after-all}
%The results from the linear probes shown in Figure~\ref{fig-fomc} are certainly not unimpressive:
Even though \emph{FOMC-RoBERTa}, which is substantially smaller than the models tested in~\citet{gurnee2023languagev2}, was not explicitly trained to uncover associations between central bank communications and the level of consumer prices, it appears that the model has distilled representations that can be used to predict inflation (although they certainly will not win any forecasting competitions). So, have we uncovered further evidence that LLMs ``aren't mere stochastic
parrots''? Has \emph{FOMC-RoBERTa} developed an intrinsic `understanding'
of the economy just by `reading' central bank communications? Thus, can economists readily adopt FOMC-RoBERTa as a domain-relevant tool?

%It is worth pointing out here that this model is substantially smaller than the models tested in \citet{gurnee2023language}. This may tempt us to ask the following questions:

% \begin{quote}
% Have we uncovered further evidence that LLMs ``aren't mere stochastic
% parrots''? Has \emph{FOMC-RoBERTa} developed an intrinsic `understanding'
% of the economy just by `reading' central bank communications?
% \end{quote}

We are having a very hard time believing that the answer to either of these questions is `yes'. To argue our case, we will now produce a counter-example demonstrating that, if anything, these findings are very much in line with the parrot metaphor. The counter-example is based on the following premise: if the results from the linear probe truly were indicative of some intrinsic `understanding' of the economy, then the probe should not be sensitive to random sentences that are most definitely not related to consumer prices.

To test this, we select the best-performing probe trained on the final-layer activations for each indicator. We then make up sentences that fall into one of these four categories: \emph{Inflation/Prices} (IP)---sentences about price inflation, \emph{Deflation/Prices} (DP)---sentences about price deflation, \emph{Inflation/Birds} (IB)---sentences about inflation in the number of birds and \emph{Deflation/Birds} (DB)---sentences about deflation in the number of birds. A sensible sentence for category DP, for example, could be: ``It is essential to bring inflation back to target to avoid drifting into deflation territory.''. Analogically, we could construct the following sentence for the DB category: ``It is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.''. While domain knowledge suggests that the former is related to actual inflation outcomes, the latter is, of course, completely independent of the level of consumer prices.

% We top this up with five sentences generated through ChatGPT, where we choose those that lead to the highest probe prediction in the direction of interest. In other words, we conduct a small manual adversarial attack on the probe.

In light of the encouraging results in Figure~\ref{fig-fomc}, we should expect the probe to predict higher levels of inflation for activations for sentences in the IP category, than for sentences in the DP category. If this was indicative of true intrinsic `understanding' as opposed to memorization, we would not expect to see any significant difference in predicted inflation levels for sentences about birds, independent of whether or not their numbers are increasing. More specifically, we would not expect the probe to predict values for sentences about birds that are substantially different from the values it can be expected to predict for actual white noise. % when using actual white noise as inputs.

To get to this last point, we also generate many probe predictions for samples of noise. Let \(f: \mathcal{A}^k \mapsto \mathcal{Y}\) denote the linear probe that maps from the \(k\)-dimensional space spanned by \(k\) first principal components of the final-layer activations to the output variable of interest (CPI growth in this case). Then we sample \(\varepsilon_i \sim \mathcal{N}(\mathbf{0},\mathbf{I}^{(k \times k)})\) for \(i \in [1,1000]\) and compute the sample average. We repeat this process \(10000\) times and compute the median-of-means to get an estimate for \(\mathbb{E}[f(\varepsilon)]=\mathbb{E}[y|\varepsilon]\), that is the predicted value of the probe conditional on random noise.

Figure~\ref{fig-attack} shows the results of this small test: it shows predicted inflation levels subtracted by \(\mathbb{E}[f(\varepsilon)]\). The median linear probe predictions for sentences about inflation and deflation are indeed substantially higher and lower, respectively than for random noise. Unfortunately, the same is true for sentences about the inflation and deflation in the number of birds, albeit to a somewhat lower degree. This finding holds for both inflation indicators and to a lesser degree also for yields at different maturities, at least qualitatively.

\section{Human Proneness to Over-Interpretation}

Linear probes and related tools from mechanistic interpretability were proposed in the context of monitoring models and diagnosing potential problems \citep{alain2018understanding}. Favorable outcomes from probes merely indicate that the model ``has learned information relevant for the property [of interest]" \citep{belinkov2021probing}. Our examples demonstrate that this is achievable even for small models, while these have certainly not developed intrinsic ``understanding'' of the world. Thus, 
%to draw conclusions about the emerging capabilities of AI models,
we argue that more conservative and rigorous tests for emerging capabilities of AI model are needed.

%that most readers will agree, have certainly not developed an intrinsic "understanding" of the world. The fact that even these models distill meaningful representations, goes to show, that more conservative and rigorous tests are needed, to draw conclusions about the emerging capabilities of AI models.

%Our examples should not be seen as an attempt to conclusively refute claims, that LLMs can develop on actual "understanding" of the world, by distilling knowledge in more compact representations. We hope that they serve to illustrate, however, that the use of diagnostic tools is potentially misleading and most definitely insufficient, in providing evidence in favor of "understanding". Linear probes and related tools from mechanistic interpretability, were proposed in the context of monitoring models and diagnosing potential problems \citep{alain2018understanding}. Favourable outcomes from probes merely indicate, that the model "has learned information relevant for the property [of interest]" \citep{belinkov2021probing}. Our examples demonstrate, that this is achievable even for small models, that most readers will agree, have certainly not developed an intrinsic "understanding" of the world. The fact that even these models distill meaningful representations, goes to show, that more conservative and rigorous tests are needed, to draw conclusions about the emerging capabilities of AI models.

Generally, humans are prone to seek patterns everywhere. Meaningful patterns have proven useful in helping us make sense of our past, navigate our present and predict the future. Although this tendency to perceive patterns likely leads to evolutionary benefits even when the perceived patterns are false \cite{foster2009evolution}, psychology has revealed a host of situations in which the ability to perceive patterns severely misfires, leading to irrational beliefs in e.g.\ the power of superstitions \cite{foster2009evolution}, conspiracy theories \cite{van2018connecting}, the paranormal \cite{muller2023linking}, gambler's fallacies \cite{ladouceur1996erroneous} and even interpreting pseudo-profound bullshit as meaningful \cite{walker2019finding}. 

We argue herein that AI research and development is a perfect storm that encourages our human biases to perceive spurious sparks of general intelligence in AI systems. When an AI system extracts patterns in the corpus not originally (thought to be) perceived during training, we can easily be misled to perceive and interpret this as the AI system having greater cognitive capabilities. We further elaborate on this by highlighting the risks of finding spurious patterns, and reviewing social science knowledge on the tendency of humans to anthropomorphize and have cognitive bias.
%. This `emergence' of understanding is more frequently observed (mistaken) thanks in part to the humongous, obscured data with which these models are trained upon.

\subsection{Spurious Relationships}
In statistics, misleading patterns are often referred to as spurious relationships: associations, often quantitatively assessed, between two or more variables that are not causally related to each other. Although the formal definition of spuriousness varies somewhat \cite{haig2003spurious}, it distinctly implies that the observation of correlations does not necessarily imply causation. Quantitative data often show non-causal associations (as humorously demonstrated on the \href{http://www.tylervigen.com/spurious-correlations}{Spurious Correlations} website), and as adept as humans are at recognizing patterns, we typically have a much harder time discerning spurious relationships from causal ones. 

A major contributor is that humans struggle to tell the difference between random and non-random sequences \cite{falk1997making}, and to generate sequences that appear random \cite{ladouceur1996erroneous}. A common issue is a lack of expectation that randomness that hints towards a causal relationship, such as correlations, will still appear at random. This leads even those trained in statistics and probability to perceive illusory correlations, correlations of inflated magnitude (see \citet{nickerson1998confirmation}), or causal relationships in data that is randomly generated \cite{zgraggen2018investigating}.


%Such as an AI system finding patterns in data not originally perceived during training
%patterns exhibited by AI systems which are mere reflections of the data used to train them, but that their developers interpret as hints at other, greater cognitive capabilities.
 



%Our society is so invested in finding patterns that today it seems we are more willing than ever to outsource this task to an Artificial Intelligence (AI): an omniscient oracle that leads us down the right path. Unfortunately, history has shown time and again that patterns are double-edged swords: if we attribute the wrong meaning to them, they may lead us nowhere at all, or worse, they may lead us down the dark roads. Despite new and increased momentum in scientific fields concerned with causal inference and discovery, I am also willing to go out on a limb and claim that we are not about to finally reach the top of Judea Pearl’s Causal Ladder through the means of Causal AI.

%I agree with the premise that in a world full of spurious relationships, causal reasoning is our only remedy. But I am very skeptical of claims that AI will magically provide that remedy. This leads me to the title and topic of this post: spurious sentience - patterns exhibited by artificial intelligence that may hint at sentience but are just reflections of the data used to train them. The article is written in response to a recent paper and claims by one of the authors, Max Tegmark, that revealed structure in the latent embeddings of Llama 2 should finally have us believe that LLMs are more than just parrots. Since this is an opinionated post, I feel that I should start with a few disclaimers:

%\begin{itemize}
%    \item I take no issue with the methodological ideas that form the foundation of the article in question: on the contrary, I think that mechanistic interpretability is an interesting and important toolkit that can help us better understand the intrinsics and behavior of opaque artificial intelligence.
%    \item The visualizations are intriguing, the code is open-sourced and the findings are interesting.
%    \item I am surprised that people are surprised by the findings: if we agree that LLMs exhibit strong capabilities that can only be connected to the patterns observed in the data they were trained with, then where exactly do you expect this information to be stored if not in the parameters of the model?\footnote{I would be very surprised—concerned even—if our search for patterns in latent spaces of capable LLMs revealed nothing at all.}
%    \item I therefore do take issue with the way that these findings are being overblown by people with clout. Perhaps the parrot metaphor should not be taken too literally either, but if anything the paper’s findings seem to support the notion that LLMs are remarkably capable of memorizing explicit and implicit knowledge contained in text.
%\end{itemize}

%\section{Background}


%\subsection{Emergence and Stochastic Parrots}
%Practically speaking, large language models (LLMs) are probabilistic representations of data on which they are trained, aimed at continuing an incomplete block of text. It is assumed that e.g. words, sentences, paragraphs, punctuation etc. appear in human-generated text according to a probability distribution \cite{arora2023theory}. Given a set of words, the task of the LLM is to draw upon its representation of human-generated text to continue that set of words: in essence, using the probability of the occurrences of words and an initial block of text to generate more text. AI-researchers have compared LLMs by observing whether the text they generate appears to solve tasks\footnote{e.g. the \href{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}{HuggingFace} LLM leaderboard}. Tasks include asking sets of questions across categories e.g. health, law, finance, and fine-tuning an LLM to check the answers \cite{lin2021truthfulqa}, asking the LLM to determine which noun a pronoun refers to in sentences where this is ambiguous \cite{sakaguchi2021winogrande}. 

%It has been shown that increasing the scale of large language models (LLMs) has been shown to result in an unexpectedly large boost in their scores on these various tasks, including scoring well on entirely new tasks. It has been shown that at a certain size, LLMs appear to hit a threshold, after which performance on a broad range of tasks increases sharply \cite{wei2022emergent}. Some theories of emergence, inspired by literature on human education, refer to the tendency of the generated text to be the correct response to a given task as a 'skill' \cite{arora2023theory}, inspiring further research to develop more challenging tasks for LLMs to complete as they routinely begin to score higher and higher on common benchmarks \cite{yu2023skill}. An explanation offered for the sudden boost in both task performance and the number of new tasks that appear to be solved is 'emergence': unpredictable qualitative differences in behavior of a system that occur with quantitative changes in that system. It has been claimed that, as the scale of the LLMs has dramatically increased, they have crossed a threshold that has resulted in a large increase in their abilities, to the point that they demonstrate 'sparks of AGI' \cite{bubeck2023sparks}. 

%An alternative explanation is that the LLMs are performing as designed, as stochastic parrots, generating strings probabilistically based on the vast text they have been trained on \cite{bender2021dangers}. Although the various tasks used to compare the various LLMs appear to be increasing in complexity, \cite{yu2023skill} acknowledges that higher scores on the tasks can be achieved by training LLMs on the material on which they will be tested ('cramming'), and that there is training-set contamination, i.e. that the vast text they are trained on contains similar examples to the tasks on which they are measured. Thus, as many of these tasks are derived from tests and textbooks commonly assigned to humans, and as much of the contents of the their training data text is unknown, metrics echo similar issues in human education: often called 'Goodhart's Law', a measure ceases to be a measure when it becomes a target, as is the case when human students taught to score high on their standardized tests to the detriment of developing the abilities the tests are supposed to measure \cite{strathern1997improving}. Further, as \citet{bender2021dangers} reviews, the interpretation of text as coherent has no objective test, relying instead on our human interpretation of the probabalistically generated output. %Although many deep learning systems' designs were inspired by the anatomy of human neurons, the neuron in computer code does not in fact resemble a human, and as such cannot deliver the cognitive functions of a human \cite{salles2020anthropomorphism, van2023reclaiming}. 

%\subsection{Claims of Artificial General Intelligence}
\subsection{Anthropomorphism}

Research on anthropomorphism has repeatedly shown the human tendency to attribute human-like characteristics to non-human agents and/or objects. These might include the weather and other natural forces, pets and other animals, gadgets and other pieces of technology~\cite{epley2007seeing}. Formally studied as early as 1944,~\cite{heider1944experimental} observed that humans can correctly interpret a narrative whose characters are abstract 2D shapes, but also that humans interpreted random movements of these shapes as having a human-like narrative. 
%Anthropomorphism was implied, because the descriptions of the 2D shapes went beyond their observable movements, and included representations of mental, emotional, and cognitive processes that resemble humans, findings that have been expanded upon substantially in the following decades (see \cite{epley2007seeing} for review). 

Relevant to AI and the degree to which it resembles AGI, anthropomorphizing may occur independently of whether such judgments are accurate, and as a matter of degree: at the weaker end, one may employ anthropomorphism as a metaphorical way of thinking or explaining, and at the stronger end one may attribute human emotions, cognition, and intelligence to AI systems. As \citet{epley2007seeing} note, literature has shown that even weak metaphorical anthropomorphism may affect how humans behave towards non-human agents.

Modern anthropomorphism theory
%of anthropomorphism
suggests there are three key components, one of which is a cognitive feature, and two of which are motivations. The first involves the easy availability of our experiences as heuristics that can be used to explain external phenomena: ``...knowledge about humans in general, or self-knowledge more specifically, functions as the known and often readily accessible base for induction about the properties of unknown agents" (p.866) \cite{epley2007seeing}. Thus, our experience as humans is an always-readily-available template to interpret the world, including non-human agent behaviors. 

This may be more so when the behaviors of that agent are made to resemble humans, which can be a benefit to the second key component of the theory: a motivational state to anthropomorphize among individuals experiencing loneliness, social isolation, or otherwise seeking social connection \cite{epley2007seeing, waytz2010social}. 
%An illustration is the example of Tom Hank's character in the film Castaway who has conversations with a ball he calls "Wilson". Those experiencing a need for social connection may begin to imbue objects, gadgets, pets etc. with human-like qualities to satisfy their social needs, anticipating the now-forming market for apps and robots to fill this function \cite{salles2020anthropomorphism}. 

The third component is the motivation as a human to be competent (effectance motivation). This is most relevant to this discussion, as it describes the need to effectively interact with our environments, including the technologies of the day \cite{epley2007seeing}. When confronted with an opaque technology, a person may interpret its behaviors using the most readily available template at hand, namely their personal human experience, in order to facilitate learning \cite{epley2007seeing, waytz2010social}. Perceiving human characteristics, motivations, emotions, and cognitive processes from one's own experiences in a technology e.g. an AI chatbot, allows for a ready template of comparison at the very least, and possibly an increase in ability to make sense of, and even predict, the agent's behaviors. This may include being placed in a position to master a certain technology, whether by incentives to learn, or fear of poor outcomes should one not manage to learn. 

These pressures extend to AI experts, as well as laypersons. In both scholarly and commercial fields, AI experts face considerable pressure to demonstrate competence in their work. Citation metrics and scholarly publications remain the primary metric for tenure and promotion, e.g. \cite{alperin2019significant}, and the number of publications in the AI field has boomed as evidenced by overall (preprint and peer-reviewed) scholarly publications\footnote{\url{https://ourworldindata.org/grapher/annual-scholarly-publications-on-artificial-intelligence?time=2010..2021} }~\cite{Maslej2023-pi}.
%, with both more than doubling from 2010 to 2020.
The adoption of techniques underlying technologies with the AI label, i.e.\ machine learning, has spread to fields beyond Computer Science, e.g.\ Astronomy, Physics, Medicine and Psychology\footnote{Retrieved 23/01/23 using the search string "TITLE-ABS-KEY ( ( machine  AND  learning )  OR  ( artificial  AND  intelligence )  OR  ai )  AND  PUBYEAR  $>$  2009  AND  PUBYEAR  $<$  2024 " from the \href{https://www.scopus.com/}{SCOPUS} database}. Outside of academia, the number of jobs requiring AI expertise increases rapidly,
%has increased severalfold, 
with demand for `Machine Learning' skills clusters having increased over 500\% from 2010 to 2020 \cite{Maslej2023-pi}. Thus, according to theory, the pressure to demonstrate AI-competence is fertile ground for anthropomorphism to occur. 


\subsection{Confirmation Bias}

Confirmation bias is generally defined as favoring interpretations of evidence that support existing beliefs or hypotheses \cite{nickerson1998confirmation}.
%Importantly, it need not be fully conscious. Rather,
Theory suggests that it is a category of implicit and unconscious processes that involve assembling one-sided evidence, and shaping it to fit one's belief. Equally important is that theory suggests these behaviors may be motivated or unmotivated, as one may selectively seek
%, attend to, or interpret
evidence in favor of a hypothesis, which one may or may not have a personal interest in supporting.

Hypotheses in present-day AI research are often implicit. Generally, these hypotheses are framed simply as a system being more accurate or efficient, compared to other systems. Where other fields, such as medicine or quantitative social sciences, would further articulate expectations in e.g.\ assigning specific conditions and considering effect sizes assigned to each competing hypothesis, in computer science and AI this is typically not done. This also may have to do with much of the published work being more of an engineering achievement, rather than a true hypothesis test seeking to explain and understand the world. % It is also not fully articulated as in other fields, but instead assigns specific conditions and in some instances numerical values or effect sizes to each competing hypothesis.
%Often framed as informally that a given system is more accurate or efficient compared to other systems, and not typically articulated as they are in other fields, with specific conditions and in some instances numerical values or effect sizes assigned to each competing hypothesis. 

However, in discussions on emerging qualities like AGI, this engineering positioning gets muddier, and more formal hypothesis testing would be justifiable: either one interprets outputs as in support of hints towards AGI (the competing hypothesis), or as merely the result of an algorithm integrating qualities from the data it was trained on (the null hypothesis). 
%However with regards to claims that outputs of a system hint towards emerging qualities like AGI, interpretations of the output necessarily become more explicit, even though the hypotheses were never fully articulated: either one interprets outputs as in support of hints towards AGI (the competing hypothesis), or as merely the result of an algorithm integrating qualities from the data it was trained on (the null hypothesis). 

Confirmation bias in hypothesis testing may manifest as a number of behaviors (\citet{nickerson1998confirmation}). Scientists may pay little to no attention to competing hypotheses or explanations, e.g. only considering the likelihood that outputs of a system support one's claims, and not the likelihood that the same outputs might occur if one's hypothesis is false. 

Similarly, bias may show when failing to articulate a sufficiently strong null hypothesis leading to a `weak' or `non-risky' experiment, a problem articulated in response to a number of scientific crises \cite{claesen2022severity}. In extreme cases, propositions may be made that cannot be falsified based on how they are formulated. If the threshold to accept a favored hypothesis is too low, observations consistent with the hypothesis are almost guaranteed, and in turn fail to severely test the claim in question. Thus, one is far more likely to show evidence in favor of their beliefs by posing weak null hypotheses. 

Related to the formulation of hypotheses is the interpretation of evidence in favor of competing hypotheses, wherein people will interpret identical evidence differently based on their beliefs. As \citet{nickerson1998confirmation} reviews, individuals may place greater emphasis or milder criticism on evidence in support of their hypothesis, and lesser emphasis and greater criticism on evidence that opposes it. 

%Further, they are under similar pressure as experts to understand and be able to use it. One such situation is the increasing demand for `AI' skills among AI non-experts. For example, job postings for individuals with skills in `GenAI' have increased dramatically since the release of ChatGPT\footnote{Retrieved 19/01/2023 from https://lightcast.io/resources/blog/generative-ai-10-19-2023}, and individuals who are in a position to adopt to the technology are projected to benefit \footnote{Retrieved 19/01/23 from https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379}. Thus, individuals in advanced economies may experience a pressure to rapidly learn how to effectively use and understand an opaque technology, that appears to behave human-like in at least some ways and by imbuing the technology with humanity, may in turn ease their learning curve. 

\section{Conclusion}
As discussed above, AI research and development outcomes can very easily be over-interpreted, both from a data perspective, and because of human biases and interests. Even academic researchers are not free from such biases. Thus, we call for the community to create explicit room for organized skepticism.

For research that seeks to explain a phenomenon, clear hypothesis articulation and strong null hypothesis formulation will be needed. If claims of human-like or superhuman intelligence are made, these should be subject to severe tests \cite{claesen2022severity} that go beyond the display of surprise. Apart from focusing on getting novel improvements upon state-of-the-art published, organizing red-teaming activities as a community may help in incentivizing and normalizing constructive adversarial questioning.

As the quest for AGI is so deeply rooted in human-like recognition, adding our voice to emerging calls to be vigilant in communication~\cite{shanahan2024talking}, we put in an explicit word of warning about the use of terminology. Many terms used in current A(G)I research (e.g.\ emergence, intelligence, learning, `better than human' performance) have a common understanding in specialized research communities, but have bigger, anthropomorphic connotations in laypersons. In fictional media, depictions of highly intelligent AI have for long been going around. In a unique study of films featuring fictional robots, defined as "...an artificial entity that can sense and act as a result of (real-world or fictional) technology...", in the 134 most highly rated science-fiction movies on IMDB, 74 out of the 108 AI-robots studied had a humanoid shape, and 68 out of those had sufficient intelligence to interact at an almost human-level \cite{saffari2021does}. The authors identify human-like communication and the ability to learn as essential abilities in the depiction of AI agents in movies. They further show a common plot: humans perceive the AI agents as inferior, despite their possession of self-awareness and the desire to survive, which fuels the central conflict of the film, wherein humanity is threatened by AI superior in both intellect and physical abilities. Moreover, it is often noted that experts and fictional content creators interact, informing and inspiring each other \cite{saffari2021does, neri2020role}.

This image also permeates present-day non-fictional writings on AI, which often heavily use anthropomorphized language (e.g.\ ``ever more powerful digital minds'' in the `Pause Giant AI Experiments' open letter~\cite{pauseai}). News outlets love a story that sells and so we witness examples of humans falling in love with their AI chatbots~\cite{replika,nytimesailove}. The same news outlets discuss the human-like responses of Microsoft's Bing search engine, which had at that point recently been adopting GPT-4\footnote{A large multimodal language model from OpenAI \url{https://openai.com/research/gpt-4}}. The article~\cite{nytimesbing}, states ``As if Bing wasn’t becoming human enough'' and goes on to claim it told them it loves them. Here, AI experts and influencers also have considerable influence on how the narrative unfolds on social media: according to~\citet{neri2020role}, actual AI-related harms did not trigger viral amplification (e.g.\ the death of an individual dying while a Tesla car was in autopilot, or the financial bankruptcy of a firm using AI technology to execute stock trades). Rather, potential risks expressed by someone perceived as having expertise and authority were amplified, such as statements made by Stephen Hawking during an interview in 2014.

We as academic researchers carry great responsibility for how the narrative will unfold, and what claims are believed. We call upon our colleagues to be explicitly mindful of this. As attractive as it may be to beat the state-of-the-art with a grander claim, let us return to the Mertonian norms, and thus safeguard our academic legitimacy in a world that only will be eager to run with made claims.

\section{Impact Statement}
This work was written out of concern that work easily recognized as `impactful' in current AI research and development, can also easily be over- and misinterpreted. As such, in the current climate of high market demand for AI innovations, we see risks of too-eager adoption in real-world applications, which may have serious societal impact. Thus, in this work we emphasize that an academic's impact also is in being able to thoroughly question made claims, and being explicitly aware of one's own biases. While the AI publishing landscape has to our feeling transformed too much into a noisy race to get exciting results in fast, we hope the research community can create more room for this type of deeper questioning.

% \section{Lessons and calls for the research community}

% \begin{itemize}
% \setlength\itemsep{1px}
%     \item be aware over-claiming is both really attractive, and we as humans (even academics) are prone to it
%     \item create explicit room for organized skepticism, especially as stakes are high. e.g.\ red teams and more severe tests, \textcolor{red}{andrew:clearly articulate hypotheses, including strong null hypotheses}
%     \item take responsibility wrt the media narrative. Note that the public may have very different interpretations and associations on words we have specifically defined within the community, but that have anthropormophic connotation (e.g.\ emergence, intelligence, learning). Scientific outcomes should not be PR press releases \textcolor{magenta}{cynthia: have there been examples of this wrt AI/AGI in the last year? So that press releases with big claims on academic work got out? Here, I also would ideally like those examples of scifi portrayal, and anthrophomorphic discussions that imply terminator-like scenarios. We can at least criticize the 'pause AI' letter of last year March, that did speak of 'minds'}
%     \item cool down the hype and uphold the Mertonian norms. Our legitimacy as academics deeply roots into this.
% \end{itemize}

% \subsection{AI in the Media}

% Lay-persons consume AI-related content in both traditional and social media, yet lack direct experience building and evaluating it. This suggests that lay-person impressions are likely to be formed by the content ingested, mediated by experts on how AI is unfolding in the real world on social media, and fictional representations in movies, TV and other traditional media. Analyzing Twitter data from 2007 to 2018, it was observed that public interest in AI dramatically increased after the first quarter of 2016 \cite{neri2020role}. Actual AI-related harms did not trigger viral amplification e.g. the death of an individual dying while a Tesla car was in autopilot, or the financial bankruptcy of a firm using AI technology to execute stock trades. Rather, potential risks expressed by Stephen Hawking during an interview in 2014 were far more likely to be expressed, and to be present in links that were shared. \citet{neri2020role} note that the 88\% of tweets classified as expressing risk described future risks rather than current and near-future harms e.g. \cite{solaiman2023evaluating}, and were greatly amplified when expressed by someone perceived as having expertise and authority. 

% Impressions are also formed by the fictional content we consume, which frequently depicts AI to be highly intelligent. In a unique study of films featuring fictional robots, defined as "...an artificial entity that can sense and act as a result of (real-world or fictional) technology...", in the 134 most highly rated science-fiction movies on IMDB, it was shown that 74 out of the 108 AI-robots studied had a humanoid shape, and 68 out of those 74 had sufficient intelligence to interact at an almost human-level \cite{saffari2021does}. The authors identify human-like communication and the ability to learn as essential abilities in the depiction of AI-agents in movies. They further show a common plot: humans perceive the AI-agents as inferior despite their possession of self-awareness and the desire to survive, which fuels the central conflict of the film wherein humanity is threatened by AI superior in both intellect and physical abilities. Moreover, it is often noted that experts and fictional content creators interact, informing and inspiring each other \cite{saffari2021does, neri2020role}. 




\bibliography{biblio}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Additional Experiments and Details}\label{appendix:autoencoder}

In this section, we present additional experimental results that we did not include in the body of the paper for the sake of brevity. We still choose to provide them as additional substantiation of our arguments here. This section also contains additional details concerning the experiment setup for our examples where applicable. 

\subsection{Are Neural Networks Born with World Maps?}

The initial feature matrix \(X^{(n \times m)}\) is made up of \(n=4,217\) and \(m=10\) features.  We add a total of \(490\) random features to \(X\) to simulate the fact that not all features ingested by Llama-2 are necessarily correlated with geographical coordinates. That yields \(500\) features in total. The training subset contains \(3,374\) randomly drawn samples, while the remaining \(843\) are held out for testing. The single hidden layer of the untrained neural network has \(400\) neurons.

%\subsection{Example: Autoencoder}\label{example-deep-learning}
\subsection{Autoencoders as Economic Growth Predictors}\label{example-deep-learning}

%So far we have considered simple linear transformations of data. One might argue that the previous examples to not really involve latent embeddings in the way that they are typically thought of in the context of deep learning. Our next example will therefore involve a deep neural network.

This is an additional example that we have not discuss in the body of the paper. Here, we build forth on an application in Economics. However, we now seek to not only predict economic growth from the yield curve, but also extract meaningful features for downstream inference tasks. For this, we will use a neural network architecture.
%, that allows us to recover a compact latent representation of the yield curve.

\subsubsection{Data}\label{data}

To estimate economic growth, we will rely on a quarterly
\href{https://fred.stlouisfed.org/series/GDPC1}{series} of the real gross domestic product (GDP) provided by the Federal Reserve Bank of St.~Louis. The data arrives in terms of levels of real GDP. In order to estimate growth, we transform the data using log differences. Since
our yield curve data is daily, we aggregate it to the
quarterly frequency by taking averages of daily yields for each maturity. We also standardize yields since deep learning models tend to perform better with standardized data \cite{gal2019standardization}. Since COVID-19 was a substantial structural break in the time series, we also filter out all observations after 2018.

\subsubsection{Model}\label{model}

%Let \(g_t\) denote growth and \(\mathbf{r}_t\) denote the matrix of aggregated Treasury yield rates at time \(t\). Then we are interested in a model for \(g_t\) conditional on \(\mathbf{r}_t\). Let \(\theta\) denote our model parameters then formally we are interested in maximizing the likelihood, \(p_{\theta}(g_t|\mathbf{r}_t)\). 

%To do this, we use a simple autoencoder architecture. With this
Using a simple autoencoder architecture, we let our model \(g_t\) denote growth and our conditional \(\mathbf{r}_t\) the matrix of aggregated Treasury yield rates at time \(t\). Finally, we let \(\theta\) denote our model parameters. Formally, we are interested in maximizing the likelihood \(p_{\theta}(g_t|\mathbf{r}_t)\). 

%To do this, we use a simple autoencoder architecture: 
The encoder consists of a single fully connected hidden layer with 32 neurons and a hyperbolic tangent activation function. The bottleneck layer connecting the encoder to the decoder, is a fully connected layer with 6 neurons. The decoder consists of two fully connected layers, each with a hyperbolic tangent activation function: the first layer consists of 32 neurons and the second layer will have the same dimension as the input data. The output layer consists of a single neuron for our output variable, \(g_t\). We train the model over 1,000 epochs to minimize mean squared error loss using the Adam optimizer~\citep{kingma2017adam}.

The in-sample fit of the model is shown in the left chart of Figure~\ref{fig-dl-results}, which shows actual GDP growth and fitted values from the autoencoder
model. The model has a large number of free parameters and captures the relationship between economic growth and the yield curve reasonably well, as expected. Since our primary goal is not out-of-sample prediction accuracy but feature extraction for inference, we use all of the available data instead of reserving a hold-out set. As discussed above, we also know that the relationship between economic growth and the yield curve is characterized by two main factors: the level and the spread. Since the model itself is fully characterized by its parameters, we would expect that these two important factors are reflected somewhere in the latent parameter space. 

\subsubsection{Linear Probe}\label{linear-probe}

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/dl.png}

}

\caption{\label{fig-dl-results}The left chart shows the actual GDP growth
and fitted values from the autoencoder model. The right chart shows the
observed average level and spread of the yield curve (solid) along with
the predicted values (in-sample) from the linear probe based on the latent embeddings
(dashed).}

\end{figure*}%

While the loss function applies most direct pressure on layers near the final output layer, any information useful for the downstream task first needs to pass through the bottleneck layer \citep{alain2018understanding}. On a per-neuron basis, the pressure to distill useful representation is therefore likely maximized there. Consequently, the bottleneck layer activations seem like a natural place to start looking for compact, meaningful representations of distilled information.  We compute and extract these activations \(A_t\) for all time periods \(t=1,...,T\). Next, we use a linear probe to regress the observed
yield curve factors on the latent embeddings. Let \(Y_t\) denote the vector containing the two factors of interest in time \(t\): \(y_{t,l}\) and \(y_{t,s}\) for the level and spread, respectively. Formally, we are interested in the following regression model: \(p_{w}(Y_t|A_t)\) where \(w\) denotes the regression
parameters. We use Ridge regression with \(\lambda\) set to \(0.1\). Using the estimated regression parameters \(\hat{w}\), we then predict the yield curve factors% from the latent embeddings
: \(\hat{Y}_t=\hat{w}^{\prime}A_t\).

The in-sample predictions of the probe are shown in the right chart of Figure~\ref{fig-dl-results}. Solid lines show the observed yield curve factors over time, while dashed lines show predicted values. We find that the latent embeddings predict the two yield curve factors reasonably well, in particular the spread. %As in all of the other examples we have presented so, we do not find this result particularly surprising. 

Did the neural network now learn an intrinsic understanding of the economic relationship between growth and the yield curve? To us, that would be too big of a statement. Still, the current form of information distillation can be useful, even beyond its intended use for monitoring models.
%It may serve as a reasonable diagnostic tool to reassure us that the model has picked up on meaningful patterns in the underlying data. But we would certainly not conclude that the model has somehow implicitly developed an intrinsic "understanding" of the economic relationship between growth and the yield curve. While we do not prescribe to this particular view, we still support the idea that this form of information distillation can be useful even beyond its intended use for monitoring models, which we will turn to next.
%\subsubsection{Potential Use-Case: Inference}\label{ok-but-truly-whats-the-point}
For example, an interesting idea could be to use the latent embeddings as features in a more traditional and interpretable econometric model. To demonstrate this, let us consider a simple linear regression model for GDP growth. We might be interested in understanding to what degree economic growth in the past is associated with economic growth today. As we might expect, linearly regressing economic growth on lagged growth, as in column (1) of Table \ref{tab-reg}, yields a statistically significant coefficient. However, this coefficient suffers from confounding bias since there are many other confounding variables at play, of which some may be readily observable and measurable, but others may not.

% A less risky/strong term than confounding bias that we could use here is omitted variable bias. Difference is nuanced: https://stats.stackexchange.com/questions/496328/difference-omitted-variable-bias-and-confounding

We e.g.\ already mentioned the relationship between interest rates and economic growth. To account for that, while keeping our regression model as parsimonious as possible, we could include the level and the spread of the US Treasury yield curve as additional regressors. While this slightly changes the estimated magnitude of the coefficient on lagged growth, the coefficients on the observed level and spread are statistically insignificant (column (2) in Table \ref{tab-reg}). This indicates that these measures may be too crude to capture valuable information about the relationship between yields and economic growth. Because we have included two additional regressors with little to no predictive power, the model fit as measured by the Bayes Information Criterium (BIC) has actually deteriorated.

Column (3) of Table \ref{tab-reg} shows the effect of instead including one of the latent embeddings that we recovered above in the regression model. In particular, we pick the one latent embedding that we have found to exhibit the most significant effect on the output variable in a separate regression of growth on all latent embeddings. The estimated coefficient on this latent factor is small in magnitude, but statistically significant. The overall model fit, as measured by the BIC has improved and the magnitude of the coefficient on lagged growth has changed quite a bit. While this is still a very incomplete toy model of economic growth, it appears that the compact latent representation we recovered can be used in order to mitigate confounding bias.

\begin{table}
\caption{Regression output for various models.}\label{tab-reg}%
\input{results/regression.tex}
\end{table}

\subsection{LLMs for Economic Sentiment Prediction}

Figures~\ref{fig-cpi} to~\ref{fig-ust-10} present average performance measures across folds for all indicators each time for the train and test set. We report the correlation between predictions and observed values (`cor'), the mean directional accuracy (`mda'), the mean squared error (`mse') and the root mean squared error (`rmse'). The model depth---as indicated by the number of the layer---increases along the horizontal axis.

Figures~\ref{fig-cpi-b} to~\ref{fig-ust-10-b} present the same performance measures, also for the baseline autoregressive model. Shaded areas show the variation across folds.
 
% ---------- CPI ----------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_probe_CPI (n_pc=128).png}

}

\caption{\label{fig-cpi}Average performance measures across folds plotted against model depth (number of layer) for the CPI for the train and test set.}

\end{figure*}%



% ---------- PPI ----------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_probe_PPI (n_pc=128).png}

}

\caption{\label{fig-ppi}Average performance measures across folds plotted against model depth (number of layer) for the PPI for the train and test set.}

\end{figure*}%

% ---------- UST (1 Mo)  ----------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_probe_UST (1 Mo) (n_pc=128).png}

}

\caption{\label{fig-ust-1}Average performance measures across folds plotted against model depth (number of layer) for the UST (1 Mo) for the train and test set.}

\end{figure*}%

% ---------- UST (1 Yr)  ----------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_probe_UST (1 Yr) (n_pc=128).png}

}

\caption{\label{fig-ust-1y}Average performance measures across folds plotted against model depth (number of layer) for the UST (1 Yr) for the train and test set.}

\end{figure*}%

% ---------- UST (10 Yr)  ----------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_probe_UST (10 Yr) (n_pc=128).png}

}

\caption{\label{fig-ust-10}Average performance measures across folds plotted against model depth (number of layer) for the UST (1 Yr) for the train and test set.}

\end{figure*}%

% -------- CPI benchmark --------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_CPI (n_pc=128).png}

}

\caption{\label{fig-cpi-b}Average performance measures across folds plotted against model depth (number of layer) for the CPI for the train and test set compared against the baseline autoregressive model. Shaded areas show the variation across folds.}

\end{figure*}%

% -------- PPI benchmark --------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_PPI (n_pc=128).png}

}

\caption{\label{fig-ppi-b}Average performance measures across folds plotted against model depth (number of layer) for the PPI for the train and test set compared against the baseline autoregressive model. Shaded areas show the variation across folds.}

\end{figure*}%

% -------- UST (1 Mo) benchmark --------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_UST (1 Mo).png}

}

\caption{\label{fig-ust-1-b}Average performance measures across folds plotted against model depth (number of layer) for the UST (1 Mo) for the train and test set compared against the baseline autoregressive model. Shaded areas show the variation across folds.}

\end{figure*}%

% -------- UST (1 Yr) benchmark --------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_UST (1 Yr).png}

}

\caption{\label{fig-ust-1y-b}Average performance measures across folds plotted against model depth (number of layer) for the UST (1 Yr) for the train and test set compared against the baseline autoregressive model. Shaded areas show the variation across folds.}

\end{figure*}%

% -------- UST (10 Yr) benchmark --------

\begin{figure*}

\centering{

\includegraphics[width=1.0\textwidth]{results/figures/measures_UST (10 Yr).png}

}

\caption{\label{fig-ust-10-b}Average performance measures across folds plotted against model depth (number of layer) for the UST (10 Yr) for the train and test set compared against the baseline autoregressive model. Shaded areas show the variation across folds.}

\end{figure*}%

\section{Toward Parrot Tests}\label{appendix:parrot}

In our experiments from Section~\ref{ex-llm}, we considered the following hypothesis tests as a minimum viable testing framework to assess if our probe results (may) provide evidence for an actual `understanding' of key economic relationships learned purely from text:

\begin{proposition}[Parrot
Test]\protect\hypertarget{prp-line}{}\label{prp-line}

~

\begin{itemize}
\setlength\itemsep{1px}
\item
  \emph{H0 (Null)}: The probe never predicts values that are statistically significantly different from \(\mathbb{E}[f(\varepsilon)]\).
\item
  \emph{H1 (Stochastic Parrots)}: The probe predicts values that are statistically significantly different from \(\mathbb{E}[f(\varepsilon)]\) for sentences related to the outcome of interest \emph{and} those that are independent (i.e. sentences in all categories).
\item
  \emph{H2 (More than Mere Stochastic Parrots)}: The probe predicts values that are statistically significantly different from \(\mathbb{E} [f(\varepsilon)]\) for sentences that are related to the outcome variable (IP and DP), but not for sentences that are independent of the outcome (IB and DB).
\end{itemize}
\end{proposition}


To be clear, if in such a test we did find substantial evidence in favour of rejecting both \emph{HO} and \emph{H1}, this would not automatically imply that \emph{H2} is true. But to even continue investigating, if based on having learned meaningful representation the underlying LLM is more than just a parrot, it should be able to pass this simple test.

In this particular case, Figure~\ref{fig-attack} demonstrates that we find some evidence to reject \emph{H0} but not \emph{H1} for \emph{FOMC-RoBERTa}. The median linear probe predictions for sentences about inflation and deflation are indeed substantially higher and lower, respectively than for random noise. Unfortunately, the same is true for sentences about the inflation and deflation in the number of birds, albeit to a somewhat lower degree. This finding holds for both inflation indicators and to a lesser degree also for yields at different maturities, at least qualitatively.

We should note that the number of sentences in each category is very small here (10), so the results in Figure~\ref{fig-attack} cannot be used to establish statistical significance. That being said, even a handful of convincing counter-examples should be enough for us to seriously question the claim, that results from linear probes provide evidence in favor of real `understanding'. In fact, even a handful of sentences for which any human annotator would easily arrive at the conclusion of independence, a prediction by the probe in either direction casts doubt.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
